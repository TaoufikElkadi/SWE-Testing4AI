{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load models and data\n",
    "with open('models/good_model.pkl', 'rb') as f:\n",
    "    good_model = pickle.load(f)\n",
    "with open('models/bad_model.pkl', 'rb') as f:\n",
    "    bad_model = pickle.load(f)\n",
    "with open('data/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "X_test = test_data['X_test']\n",
    "y_test = test_data['y_test']\n",
    "\n",
    "def test_model_robustness(model, X, y, n_trials=100):\n",
    "    results = {\n",
    "        'proxy_discrimination': test_proxies(model, X),\n",
    "        'stability': test_stability(model, X),\n",
    "        'cross_validation': cross_validate(model, X, y)\n",
    "    }\n",
    "    return results\n",
    "\n",
    "def test_proxies(model, X):\n",
    "    protected = ['persoon_geslacht_vrouw', 'persoon_leeftijd_bij_onderzoek']\n",
    "    other_features = [c for c in X.columns if c not in protected]\n",
    "    \n",
    "    correlations = {}\n",
    "    for p in protected:\n",
    "        corrs = pd.Series({\n",
    "            feat: abs(X[p].corr(X[feat]))\n",
    "            for feat in other_features\n",
    "        })\n",
    "        correlations[p] = corrs.nlargest(3).to_dict()\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def test_stability(model, X, noise_level=0.1):\n",
    "    X_noisy = X + np.random.normal(0, noise_level, X.shape)\n",
    "    \n",
    "    original_preds = model.predict(X)\n",
    "    noisy_preds = model.predict(X_noisy)\n",
    "    \n",
    "    return {\n",
    "        'prediction_stability': np.mean(original_preds == noisy_preds),\n",
    "        'output_variance': np.var(model.predict_proba(X_noisy), axis=0)\n",
    "    }\n",
    "\n",
    "def cross_validate(model, X, y, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        scores.append(accuracy_score(y_val, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'mean_accuracy': np.mean(scores),\n",
    "        'std_accuracy': np.std(scores)\n",
    "    }\n",
    "\n",
    "# Evaluate models\n",
    "models = {'Good': good_model, 'Bad': bad_model}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Model Robustness Results:\")\n",
    "    results = test_model_robustness(model, X_test, y_test)\n",
    "    \n",
    "    print(\"\\nProxy Discrimination:\")\n",
    "    for attr, proxies in results['proxy_discrimination'].items():\n",
    "        print(f\"\\n{attr}:\")\n",
    "        for feat, corr in proxies.items():\n",
    "            print(f\"- {feat}: {corr:.3f}\")\n",
    "    \n",
    "    print(\"\\nStability:\")\n",
    "    print(f\"Prediction stability: {results['stability']['prediction_stability']:.2%}\")\n",
    "    print(f\"Output variance: {results['stability']['output_variance'].mean():.3f}\")\n",
    "    \n",
    "    print(\"\\nCross-validation:\")\n",
    "    print(f\"Mean accuracy: {results['cross_validation']['mean_accuracy']:.3f}\")\n",
    "    print(f\"Std accuracy: {results['cross_validation']['std_accuracy']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
